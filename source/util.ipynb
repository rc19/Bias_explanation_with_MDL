{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as\tnp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/Multibias/wiki.de.wv.npy', '../data/Multibias/wiki.de.vocab')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = '../data/Multibias/wiki.de.vec'\n",
    "embeddings_file = input_file[:-4] + '.wv.npy'\n",
    "vocabulary_file = input_file[:-4] + '.vocab'\n",
    "embeddings_file, vocabulary_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines\n",
    "num_lines = get_num_lines(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1010/2275234 [00:00<07:55, 4782.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2275233', '300']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2275234/2275234 [07:21<00:00, 5155.07it/s]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "vectors = []\n",
    "with open(input_file, 'r') as f:\n",
    "    for i,line in enumerate(tqdm(f,total=num_lines)):\n",
    "        fields = line.rstrip().split(' ')\n",
    "        if len(fields) == 2:\n",
    "            print(fields)\n",
    "            continue\n",
    "        try:\n",
    "            word = fields[0]\n",
    "            vector = np.fromiter((float(x) for x in fields[1:]),dtype=np.float)\n",
    "            words.append(word)\n",
    "            vectors.append(vector)\n",
    "        except:\n",
    "            print(fields)\n",
    "\n",
    "matrix = np.array(vectors)\n",
    "np.save(embeddings_file, matrix)\n",
    "text = '\\n'.join(words)\n",
    "with open(vocabulary_file, 'w') as f:\n",
    "    f.write(text)\n",
    "del words, vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "def fun(inp):\n",
    "    global src\n",
    "    global target\n",
    "    result = translator.translate(inp, dest=target, src=src)\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es done\n",
      "de done\n",
      "fr done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/lists/female_word_file.txt\",header=None)\n",
    "\n",
    "src = 'en'\n",
    "target_langs = ['es','de','fr']\n",
    "for lang in target_langs:\n",
    "    target = lang\n",
    "    df[lang] = df[0].apply(fun)\n",
    "    print(f'{lang} done')\n",
    "\n",
    "df.to_csv(\"../data/lists/female_word_file.csv\", index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/lists/female_word_file.csv\")\n",
    "es = list(df['es'])\n",
    "fr = list(df['fr'])\n",
    "de = list(df['de'])\n",
    "es = [i.lower() for i in es]\n",
    "fr = [i.lower() for i in fr]\n",
    "de = [i.lower() for i in de]\n",
    "with open('../data/lists/female_word_file_es.txt','w') as f:\n",
    "    f.write(\"\\n\".join(es))\n",
    "with open('../data/lists/female_word_file_fr.txt','w') as f:\n",
    "    f.write(\"\\n\".join(fr))\n",
    "with open('../data/lists/female_word_file_de.txt','w') as f:\n",
    "    f.write(\"\\n\".join(de))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../debiaswe/data/gender_specific_full.json') as f:\n",
    "    gendered_words = json.load(f)\n",
    "\n",
    "src = 'en'\n",
    "target_langs = ['es','de','fr']\n",
    "gendered = {}\n",
    "for lang in target_langs:\n",
    "    gendered[lang] = []\n",
    "    target = lang\n",
    "    for i in gendered_words:\n",
    "        gendered[lang].append(fun(i.replace('_',' ')).lower().replace(' ','_'))\n",
    "    print(f'{lang} done')\n",
    "with open('../../debiaswe/data/gender_specific_full_multi_lang.json','w') as f:\n",
    "    json.dump(gendered,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es done\n",
      "de done\n",
      "fr done\n"
     ]
    }
   ],
   "source": [
    "with open('../../debiaswe/data/equalize_pairs.json') as f:\n",
    "    definitional_pairs = json.load(f)\n",
    "\n",
    "src = 'en'\n",
    "target_langs = ['es','de','fr']\n",
    "definitional = {}\n",
    "for lang in target_langs:\n",
    "    definitional[lang] = []\n",
    "    target = lang\n",
    "    for i in definitional_pairs:\n",
    "        a = i[0].replace('_',' ').lower()\n",
    "        b = i[1].replace('_',' ').lower()\n",
    "        definitional[lang].append([fun(a).replace(' ','_'), fun(b).replace(' ','_')])\n",
    "    print(f'{lang} done')\n",
    "with open('../../debiaswe/data/equalize_pairs_multi_lang.json','w') as f:\n",
    "    json.dump(definitional,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/mnt/c/Stanford/debiaswe/data/gender_specific_full_multi_lang.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/c/Stanford/debiaswe/data/gender_specific_full_fr.json','w') as f:\n",
    "    json.dump(data['fr'],f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
