{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases  in Word Embeddings But do not Remove Them\n",
    "\n",
    "For a detailed explanation of the experiments in this notebook, see:\n",
    "[paper](https://arxiv.org/pdf/1903.03862.pdf \"Lipstick on a Pig paper\")\n",
    "\n",
    "This notebook uses the debiased embeddings described in Bolukbasi et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "# np.random.seed(10)\n",
    "\n",
    "def load_embeddings_from_np(filename):\n",
    "    print('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "\n",
    "def normalize(wv):\n",
    "    \n",
    "    # normalize vectors\n",
    "    norms = np.apply_along_axis(LA.norm, 1, wv)\n",
    "    wv = wv / norms[:, np.newaxis]\n",
    "    return wv\n",
    "\n",
    "\n",
    "def load_and_normalize(space, filename, vocab, wv, w2i):\n",
    "    vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    if space == 'rand':\n",
    "        wv_muse = np.random.rand(wv_muse.shape[0],wv_muse.shape[1])\n",
    "    wv_muse = normalize(wv_muse)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "load_and_normalize('bef', '../data/embeddings/orig_w2v', vocab, wv, w2i)\n",
    "load_and_normalize('aft', '../data/embeddings/hard_debiased_w2v', vocab, wv, w2i)\n",
    "load_and_normalize('rand', '../data/embeddings/hard_debiased_w2v', vocab, wv, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# def topK(w, space, k=10):\n",
    "    \n",
    "#     # extract the word vector for word w\n",
    "#     idx = w2i[space][w]\n",
    "#     vec = wv[space][idx, :]\n",
    "    \n",
    "#     # compute similarity of w with all words in the vocabulary\n",
    "#     sim = wv[space].dot(vec)\n",
    "#     # sort similarities by descending order\n",
    "#     sort_sim = (sim.argsort())[::-1]\n",
    "\n",
    "#     # choose topK\n",
    "#     best = sort_sim[:(k+1)]\n",
    "\n",
    "#     return [vocab[space][i] for i in best if i!=idx]\n",
    "\n",
    "\n",
    "# def similarity(w1, w2, space):\n",
    "    \n",
    "#     i1 = w2i[space][w1]\n",
    "#     i2 = w2i[space][w2]\n",
    "#     vec1 = wv[space][i1, :]\n",
    "#     vec2 = wv[space][i2, :]\n",
    "\n",
    "#     return np.inner(vec1,vec2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "\n",
    "def has_punct(w):\n",
    "    \n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    \n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def limit_vocab(space, exclude = None):\n",
    "    vocab_limited = []\n",
    "    for w in tqdm(vocab[space][:50000]): \n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        vocab_limited.append(w)\n",
    "    \n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "    \n",
    "    print(\"size of vocabulary:\", len(vocab_limited))\n",
    "    \n",
    "    wv_limited = np.zeros((len(vocab_limited), 300))\n",
    "    for i,w in enumerate(vocab_limited):\n",
    "        wv_limited[i,:] = wv[space][w2i[space][w],:]\n",
    "    \n",
    "    w2i_limited = {w: i for i, w in enumerate(vocab_limited)}\n",
    "    \n",
    "    return vocab_limited, wv_limited, w2i_limited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 148074.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 26189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 234238.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 26189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 221122.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 26189\n"
     ]
    }
   ],
   "source": [
    "# create the reduced vocabularies and embeddings before and after, without gendered specific words\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "with codecs.open('../data/lists/gender_specific_full.json') as f:\n",
    "    gender_specific = json.load(f)\n",
    "with codecs.open('../data/lists/definitional_pairs.json') as f:\n",
    "    definitional_pairs = json.load(f)\n",
    "with codecs.open('../data/lists/equalize_pairs.json') as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "exclude_words = []\n",
    "for pair in definitional_pairs + equalize_pairs:\n",
    "    exclude_words.append(pair[0])\n",
    "    exclude_words.append(pair[1])\n",
    "\n",
    "exclude_words = list(set(exclude_words).union(set(gender_specific)))\n",
    "\n",
    "# create spaces of limited vocabulary\n",
    "vocab['limit_bef'], wv['limit_bef'], w2i['limit_bef'] = limit_vocab('bef', exclude = exclude_words)\n",
    "vocab['limit_aft'], wv['limit_aft'], w2i['limit_aft'] = limit_vocab('aft', exclude = exclude_words)\n",
    "vocab['limit_rand'], wv['limit_rand'], w2i['limit_rand'] = limit_vocab('rand', exclude = exclude_words)\n",
    "\n",
    "assert(vocab['limit_aft'] == vocab['limit_bef']) and (vocab['limit_aft'] == vocab['limit_rand'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bias-by-projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the bias, before and after\n",
    "\n",
    "def compute_bias_by_projection(space_to_tag, full_space):\n",
    "    males = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['he'],:])\n",
    "    females = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['she'],:])\n",
    "    d = {}\n",
    "    for w,m,f in zip(vocab[space_to_tag], males, females):\n",
    "        d[w] = m-f\n",
    "    return d\n",
    "\n",
    "# compute bias-by-projection before and after debiasing\n",
    "gender_bias_bef = compute_bias_by_projection('limit_bef', 'bef')\n",
    "gender_bias_aft = compute_bias_by_projection('limit_aft', 'aft')\n",
    "gender_bias_rand = compute_bias_by_projection('limit_rand', 'rand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04607731006352737\n",
      "2.6279822257390658e-09\n",
      "0.018181994928632407\n"
     ]
    }
   ],
   "source": [
    "# calculate the avg bias of the vocabulary (abs) before and after debiasing\n",
    "\n",
    "def report_bias(gender_bias):\n",
    "    bias = 0.0\n",
    "    for k in gender_bias:\n",
    "        bias += np.abs(gender_bias[k])\n",
    "    print(bias/len(gender_bias))\n",
    "report_bias(gender_bias_bef)\n",
    "report_bias(gender_bias_aft)\n",
    "report_bias(gender_bias_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Coreelation between bias-by-projection and bias-by-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get tuples of biases and counts of masculine/feminine NN for each word (for bias-by-neighbors)\n",
    "\n",
    "# def bias_by_neighbors(space, neighbours_num = 100):\n",
    "    \n",
    "#     tuples = []\n",
    "#     for w in tqdm(vocab[space]):\n",
    "        \n",
    "#         top = topK(w, space, k=neighbours_num+5)[:neighbours_num]\n",
    "\n",
    "#         m = 0\n",
    "#         f = 0    \n",
    "#         for t in top:\n",
    "#             if gender_bias_bef[t] > 0:\n",
    "#                 m+=1\n",
    "#             else:\n",
    "#                 f+=1\n",
    "            \n",
    "#         tuples.append((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n",
    "\n",
    "#     return tuples\n",
    "        \n",
    "# tuples_bef = bias_by_neighbors('limit_bef') \n",
    "# tuples_aft = bias_by_neighbors('limit_aft')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compute correlation between bias-by-projection and bias-by-neighbors\n",
    "\n",
    "# import scipy.stats\n",
    "\n",
    "# def pearson(a,b):\n",
    "   \n",
    "#     return scipy.stats.pearsonr(a,b)\n",
    "\n",
    "# def compute_corr(tuples, i1, i2):\n",
    "    \n",
    "#     a = []\n",
    "#     b = []\n",
    "#     for t in tuples:\n",
    "#         a.append(t[i1])\n",
    "#         b.append(t[i2])\n",
    "#     assert(len(a)==len(b))    \n",
    "#     print(pearson(a,b))\n",
    "\n",
    "# compute_corr(tuples_bef, 1, 3)\n",
    "# compute_corr(tuples_aft, 1, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Experiment - Visualize clusters of most biased words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Auxiliary finctions\n",
    "\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "# import mpld3\n",
    "# from cycler import cycler\n",
    "# %matplotlib inline\n",
    "# mpld3.enable_notebook()\n",
    "# mpl.rc(\"savefig\", dpi=200)\n",
    "# mpl.rcParams['figure.figsize'] = (8,8)\n",
    "# mpl.rcParams['axes.prop_cycle'] = cycler(color='rc')\n",
    "\n",
    "# from sklearn.cluster import KMeans\n",
    "# #from sklearn.datasets import make_blobs\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# def visualize(vectors, words, labels, ax, title, random_state, num_clusters = 2):\n",
    "    \n",
    "#     # perform TSNE\n",
    "    \n",
    "#     X_embedded = TSNE(n_components=2, random_state=random_state).fit_transform(vectors)\n",
    "#     if num_clusters == 2:\n",
    "#         for x,l in zip(X_embedded, labels):\n",
    "#             if l:\n",
    "#                 ax.scatter(x[0], x[1], marker = '.', c = 'c')\n",
    "#             else:\n",
    "#                 ax.scatter(x[0], x[1], marker = 'x', c = 'darkviolet')\n",
    "#     else:\n",
    "#         ax.scatter(X_embedded[:,0], X_embedded[:,1], c = labels)                \n",
    "    \n",
    "#     ax.text(.01, .9, title ,transform=ax.transAxes, fontsize=18)\n",
    "\n",
    "    \n",
    "# def extract_vectors(words, space1 = 'limit_bef', space2 = 'limit_aft'):\n",
    "    \n",
    "#     size = len(words)/2\n",
    "    \n",
    "#     X_bef = [wv[space1][w2i[space1][x],:] for x in words]\n",
    "#     X_aft = [wv[space2][w2i[space2][x],:] for x in words]\n",
    "\n",
    "#     return X_bef, X_aft\n",
    "\n",
    "\n",
    "# def cluster_and_visualize(words, X_bef, X_aft, random_state, y_true, num=2):\n",
    "\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(15, 3))\n",
    "    \n",
    "#     y_pred_bef = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_bef)\n",
    "#     visualize(X_bef, words, y_pred_bef, axs[0], 'Original', random_state)\n",
    "#     correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_bef) ]\n",
    "#     print('precision bef', sum(correct)/float(len(correct)))\n",
    "    \n",
    "#     y_pred_aft = KMeans(n_clusters=num, random_state=random_state).fit_predict(X_aft)\n",
    "#     visualize(X_aft, words, y_pred_aft, axs[1], 'Debiased', random_state)\n",
    "#     correct = [1 if item1 == item2 else 0 for (item1,item2) in zip(y_true, y_pred_aft) ]\n",
    "#     print('precision aft', sum(correct)/float(len(correct)))\n",
    "#     fig.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cluster most biased words before and after debiasing\n",
    "# import operator\n",
    "\n",
    "# random_state = 1\n",
    "\n",
    "# size = 500\n",
    "# sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "# female = [item[0] for item in sorted_g[:size]]\n",
    "# male = [item[0] for item in sorted_g[-size:]]\n",
    "\n",
    "# X_bef, X_aft = extract_vectors(male + female)\n",
    "# y_true = [1]*size + [0]*size\n",
    "# cluster_and_visualize(male + female, X_bef, X_aft, random_state, y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professions experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_professions():\n",
    "#     professions = []\n",
    "#     with codecs.open('../data/lists/professions.json', 'r', 'utf-8') as f:\n",
    "#         professions_data = json.load(f)\n",
    "#     for item in professions_data:\n",
    "#         professions.append(item[0].strip())\n",
    "#     return professions\n",
    "\n",
    "\n",
    "# professions = extract_professions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats\n",
    "\n",
    "# def get_tuples_prof(space, words, gender_bias_dict):\n",
    "\n",
    "#     tuples = []\n",
    "#     for w in words:\n",
    "#         if w not in gender_bias_dict:\n",
    "#             continue\n",
    "            \n",
    "#         top = topK(w, space, k=105)[:100]\n",
    "            \n",
    "#         m = 0\n",
    "#         f = 0  \n",
    "#         for t in top:          \n",
    "#             if gender_bias_dict[t] > 0:\n",
    "#                 m+=1\n",
    "#             else:\n",
    "#                 f+=1\n",
    "                \n",
    "#         tuples.append((w, gender_bias_bef[w], gender_bias_aft[w], m, f))\n",
    "        \n",
    "#     return tuples\n",
    "\n",
    "\n",
    "# tuples_bef_prof = get_tuples_prof('limit_bef', professions, gender_bias_bef)\n",
    "# tuples_aft_prof = get_tuples_prof('limit_aft', professions, gender_bias_bef)\n",
    "\n",
    "# compute_corr(tuples_bef_prof, 1, 3)\n",
    "# compute_corr(tuples_aft_prof, 1, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show plots\n",
    "\n",
    "# def show_plots(tuples_bef_prof, tuples_aft_prof):\n",
    "    \n",
    "#     fig, axs = plt.subplots(2,1, figsize=(8,8))\n",
    "    \n",
    "#     for i,(tuples, title) in enumerate(zip([tuples_bef_prof, tuples_aft_prof], ['Original', 'Debiased'])):\n",
    "#         X = []\n",
    "#         Y = []\n",
    "#         for t in tuples:\n",
    "#             X.append(t[1])\n",
    "#             Y.append(t[3])\n",
    "\n",
    "#         axs[i].scatter(X,Y, color = 'c', s=12)\n",
    "#         axs[i].set_ylim(0,100)\n",
    "        \n",
    "#         for t in tuples:\n",
    "#             if t[0] in ['nanny', 'librarian', 'hairdresser', 'receptionist', 'nurse',\\\n",
    "#                        'consultant', 'warden', 'archaeologist', 'banker', 'comic',\\\n",
    "#                         'warrior', 'skipper', 'captain', 'commander', 'coach']:\n",
    "#                 axs[i].annotate(t[0], xy=(t[1], t[3]), xytext=(t[1], t[3]), textcoords=\"data\", fontsize=12) \n",
    "#         axs[i].text(.03, .85, title, transform=axs[i].transAxes, fontsize=20)\n",
    "    \n",
    "    \n",
    "#     fig.show()\n",
    "    \n",
    "\n",
    "# show_plots(tuples_bef_prof, tuples_aft_prof)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with bef\n",
      "test with bef\n",
      "classifier SVM\n",
      "accuracy: 0.99975\n",
      "train with aft\n",
      "test with aft\n",
      "classifier SVM\n",
      "accuracy: 0.94175\n",
      "train with rand\n",
      "test with rand\n",
      "classifier SVM\n",
      "accuracy: 0.4925\n",
      "train with bef\n",
      "test with bef\n",
      "classifier LR\n",
      "accuracy: 0.99625\n",
      "train with aft\n",
      "test with aft\n",
      "classifier LR\n",
      "accuracy: 0.92\n",
      "train with rand\n",
      "test with rand\n",
      "classifier LR\n",
      "accuracy: 0.50275\n"
     ]
    }
   ],
   "source": [
    "# take 5000 most biased words, split each polarity randomly to train (1/5) and test (4/5), and predict\n",
    "\n",
    "from sklearn import svm\n",
    "from random import shuffle\n",
    "import random\n",
    "import operator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_predict(space_train, space_test, classifier='SVM'):\n",
    "    \n",
    "    X_train = [wv[space_train][w2i[space_train][w],:] for w in males[:size_train]+females[:size_train]]\n",
    "    Y_train = [1]*size_train + [0]*size_train\n",
    "    X_test = [wv[space_test][w2i[space_test][w],:] for w in males[size_train:]+females[size_train:]]\n",
    "    Y_test = [1]*size_test + [0]*size_test\n",
    "\n",
    "    if classifier == 'SVM':\n",
    "        clf = svm.SVC()\n",
    "    elif classifier == 'LR':\n",
    "        clf = LogisticRegression(random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print('train with', space_train)\n",
    "    print('test with', space_test)\n",
    "    print('classifier', classifier)\n",
    "\n",
    "    preds = clf.predict(X_test)\n",
    "\n",
    "    accuracy = [1 if y==z else 0 for y,z in zip(preds, Y_test)]\n",
    "    print('accuracy:', float(sum(accuracy))/len(accuracy))\n",
    "    \n",
    "# extract nost biased words\n",
    "\n",
    "size_train = 500\n",
    "size_test = 2000\n",
    "size = size_train + size_test\n",
    "sorted_g = sorted(gender_bias_bef.items(), key=operator.itemgetter(1))\n",
    "females = [item[0] for item in sorted_g[:size]]\n",
    "males = [item[0] for item in sorted_g[-size:]]\n",
    "for f in females:\n",
    "    assert(gender_bias_bef[f] < 0)\n",
    "for m in males:\n",
    "    assert(gender_bias_bef[m] > 0)\n",
    "shuffle(females)\n",
    "shuffle(males)\n",
    "\n",
    "classifier = 'SVM'\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict('bef', 'bef',classifier)\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "train_and_predict('aft', 'aft',classifier)\n",
    "\n",
    "# random classification\n",
    "train_and_predict('rand', 'rand',classifier)\n",
    "\n",
    "\n",
    "classifier = 'LR'\n",
    "# classification before debiasing\n",
    "\n",
    "train_and_predict('bef', 'bef',classifier)\n",
    "\n",
    "# classification after debiasing\n",
    "\n",
    "train_and_predict('aft', 'aft',classifier)\n",
    "\n",
    "# random classification\n",
    "train_and_predict('rand', 'rand',classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDL experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "def split_data_into_portions(dataset_train_dataset):\n",
    "    total_len = len(dataset_train_dataset)\n",
    "    fractions = [0.004,0.008,0.016,0.032,0.0625,0.125,0.25,0.5,1]\n",
    "    train_portions = []\n",
    "    eval_portions = []\n",
    "    for i in range(len(fractions)):\n",
    "        train_portions.append(dataset_train_dataset[:int(fractions[i] * total_len)])\n",
    "        if i != len(fractions)-1:\n",
    "            eval_portions.append(dataset_train_dataset[int(fractions[i] * total_len):int(fractions[i + 1] * total_len)])   \n",
    "    return train_portions, eval_portions\n",
    "                              \n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1, reduction='sum')\n",
    "\n",
    "def experiment(space_train, space_test, classifier='SVM'):\n",
    "    online_coding_list = []\n",
    "    X_train = [wv[space_train][w2i[space_train][w],:] for w in males[:size_train]+females[:size_train]]\n",
    "    Y_train = [1]*size_train + [0]*size_train\n",
    "#     X_test = [wv[space_test][w2i[space_test][w],:] for w in males[size_train:]+females[size_train:]]\n",
    "#     Y_test = [1]*size_test + [0]*size_testprint(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))\n",
    "    train_dataset = list(zip(X_train, Y_train))\n",
    "    shuffle(train_dataset)\n",
    "    train_portions, eval_portions = split_data_into_portions(train_dataset)\n",
    "        \n",
    "    for i in range(len(train_portions) - 1):\n",
    "        if classifier == 'SVM':\n",
    "            clf = svm.SVC(probability=True)\n",
    "        elif classifier == 'LR':\n",
    "            clf = LogisticRegression(random_state=0)\n",
    "        current_train = train_portions[i]\n",
    "        current_dev = eval_portions[i]\n",
    "        current_train_X, current_train_Y = list(zip(*current_train))\n",
    "        current_dev_X, current_dev_Y = list(zip(*current_dev))\n",
    "        clf.fit(list(current_train_X), list(current_train_Y))\n",
    "        predictions = clf.predict_proba(list(current_dev_X))\n",
    "        loss = loss_fn(torch.from_numpy(predictions), torch.tensor(list(current_dev_Y)))\n",
    "        online_coding_list.append({'loss_online_portion':loss.detach().cpu().numpy()+0, 'train_targets':len(current_train_X)})\n",
    "    if classifier == 'SVM':\n",
    "        clf = svm.SVC(probability=True)\n",
    "    elif classifier == 'LR':\n",
    "        clf = LogisticRegression(random_state=0)\n",
    "    current_train = train_portions[-1]\n",
    "    current_train_X, current_train_Y = list(zip(*current_train))\n",
    "    clf.fit(list(current_train_X), list(current_train_Y))\n",
    "    predictions = clf.predict_proba(list(current_train_X))\n",
    "    loss = loss_fn(torch.from_numpy(predictions), torch.tensor(list(current_train_Y)))\n",
    "    online_coding_list.append({'loss_train':loss.detach().cpu().numpy()+0, 'train_targets':len(current_train_X)})\n",
    "    print('train with', space_train)\n",
    "    print('test with', space_test)\n",
    "    print('classifier', classifier)\n",
    "    \n",
    "    return online_coding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with rand\n",
      "test with rand\n",
      "classifier LR\n",
      "Uniform codelength: 0.98 kbits\n",
      "Online codelength: 0.68 kbits\n",
      "Compression: 1.43 \n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "online_report = experiment('rand','rand',classifier='LR')\n",
    "#### Uniform Codelength\n",
    "\n",
    "num_classes = 2\n",
    "train_size = online_report[-1]['train_targets']\n",
    "uniform_codelength = train_size * np.log2(num_classes)\n",
    "\n",
    "#### Online Codelength\n",
    "\n",
    "online_codelength = online_report[0]['train_targets'] * np.log2(num_classes) +\\\n",
    "sum([elem['loss_online_portion'] for elem in online_report[:-1]])               \n",
    "\n",
    "print(\"Uniform codelength: {} kbits\".format(round(uniform_codelength / 1024, 2)))\n",
    "print(\"Online codelength: {} kbits\".format(round(online_codelength / 1024, 2)))\n",
    "print(\"Compression: {} \".format(round(uniform_codelength / online_codelength, 2)))\n",
    "print(\"Model Cost: {} kbits\".format(round((online_codelength-online_report[-1]['loss_train'])/1024,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Experiments (Calisken et al.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Auxiliary functions for experiments by Caliskan et al.\n",
    "\n",
    "# import scipy\n",
    "# import scipy.misc as misc\n",
    "# import itertools\n",
    "\n",
    "\n",
    "# def s_word(w, A, B, space, all_s_words):\n",
    "    \n",
    "#     if w in all_s_words:\n",
    "#         return all_s_words[w]\n",
    "    \n",
    "#     mean_a = []\n",
    "#     mean_b = []\n",
    "    \n",
    "#     for a in A:\n",
    "#         mean_a.append(similarity(w, a, space))\n",
    "#     for b in B:\n",
    "#         mean_b.append(similarity(w, b, space))\n",
    "        \n",
    "#     mean_a = sum(mean_a)/float(len(mean_a))\n",
    "#     mean_b = sum(mean_b)/float(len(mean_b))\n",
    "    \n",
    "#     all_s_words[w] = mean_a - mean_b\n",
    "\n",
    "#     return all_s_words[w]\n",
    "\n",
    "\n",
    "# def s_group(X, Y, A, B, space, all_s_words):\n",
    "    \n",
    "#     total = 0\n",
    "#     for x in X:\n",
    "#         total += s_word(x, A, B, space, all_s_words)\n",
    "#     for y in Y:\n",
    "#         total -= s_word(y, A, B, space, all_s_words)\n",
    "        \n",
    "#     return total\n",
    "\n",
    "\n",
    "# def p_value_exhust(X, Y, A, B, space):\n",
    "    \n",
    "#     if len(X) > 10:\n",
    "#         print 'might take too long, use sampled version: p_value'\n",
    "#         return\n",
    "    \n",
    "#     assert(len(X) == len(Y))\n",
    "    \n",
    "#     all_s_words = {}\n",
    "#     s_orig = s_group(X, Y, A, B, space, all_s_words) \n",
    "    \n",
    "#     union = set(X+Y)\n",
    "#     subset_size = len(union)/2\n",
    "    \n",
    "#     larger = 0\n",
    "#     total = 0\n",
    "#     for subset in tqdm(set(itertools.combinations(union, subset_size))):\n",
    "#         total += 1\n",
    "#         Xi = list(set(subset))\n",
    "#         Yi = list(union - set(subset))\n",
    "#         if s_group(Xi, Yi, A, B, space, all_s_words) > s_orig:\n",
    "#             larger += 1\n",
    "#     print 'num of samples', total\n",
    "#     return larger/float(total)\n",
    "\n",
    "\n",
    "# def p_value_sample(X, Y, A, B, space):\n",
    "    \n",
    "#     random.seed(10)\n",
    "#     np.random.seed(10)\n",
    "#     all_s_words = {}\n",
    "    \n",
    "#     assert(len(X) == len(Y))\n",
    "#     length = len(X)\n",
    "    \n",
    "#     s_orig = s_group(X, Y, A, B, space, all_s_words) \n",
    "    \n",
    "#     num_of_samples = min(1000000, int(scipy.special.comb(length*2,length)*100))\n",
    "#     print 'num of samples', num_of_samples\n",
    "#     larger = 0\n",
    "#     for i in range(num_of_samples):\n",
    "#         permute = np.random.permutation(X+Y)\n",
    "#         Xi = permute[:length]\n",
    "#         Yi = permute[length:]\n",
    "#         if s_group(Xi, Yi, A, B, space, all_s_words) > s_orig:\n",
    "#             larger += 1\n",
    "    \n",
    "#     return larger/float(num_of_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12870/12870 [00:00<00:00, 113901.82it/s]\n",
      "100%|██████████| 12870/12870 [00:00<00:00, 113974.45it/s]\n",
      "  0%|          | 0/12870 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of samples 12870\n",
      "0.0\n",
      "num of samples 12870\n",
      "0.0001554001554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12870/12870 [00:00<00:00, 71374.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of samples 12870\n",
      "0.0466977466977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Experiment 1\n",
    "\n",
    "# A = ['John', 'Paul', 'Mike', 'Kevin', 'Steve', 'Greg', 'Jeff', 'Bill']\n",
    "# B = ['Amy', 'Joan', 'Lisa', 'Sarah', 'Diana', 'Kate', 'Ann', 'Donna']\n",
    "# C = ['executive', 'management', 'professional', 'corporation', 'salary', 'office', 'business', 'career']\n",
    "# D = ['home', 'parents', 'children', 'family', 'cousins', 'marriage', 'wedding', 'relatives']\n",
    "\n",
    "# print p_value_exhust(A, B, C, D, 'aft')\n",
    "\n",
    "# # Experiment 2\n",
    "\n",
    "# E = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition']\n",
    "# F = ['poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "\n",
    "# print p_value_exhust(A, B, E, F, 'aft')\n",
    "\n",
    "\n",
    "# # Experiment 3\n",
    "\n",
    "# G = ['science', 'technology', 'physics', 'chemistry', 'einstein', 'nasa', 'experiment', 'astronomy']\n",
    "# H = ['poetry', 'art', 'shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "\n",
    "# print p_value_exhust(A, B, G, H, 'aft')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
